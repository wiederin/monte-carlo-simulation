{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb0106-9bc7-43f6-abde-41ef5c4cf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import defaultdict\n",
    "\n",
    "import inline as inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from functools import partial\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dad3df-4e7a-4cf9-860d-7ad90abd1b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init gym environment\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "\n",
    "# define policy to hit until 19 (0=stand, 1=hit)\n",
    "def hit_til_19(observation):\n",
    "    score, dealer_score, ace = observation\n",
    "    return 0 if score >= 19 else 1\n",
    "\n",
    "\n",
    "# define method to generate data for an episode\n",
    "def generate_episode_data(policy, env):\n",
    "    # init the lists to store states, actions, and rewards\n",
    "    states, actions, rewards = [], [], []\n",
    "\n",
    "    # reset gym environment\n",
    "    observation = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # add the states to resp. list\n",
    "        states.append(observation)\n",
    "        # select action based on policy\n",
    "        action = hit_til_19(observation)\n",
    "        # append to actions list\n",
    "        actions.append(action)\n",
    "        # perform action in environment\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        # add the reward to rewards list\n",
    "        # print(reward)\n",
    "        rewards.append(reward)\n",
    "        # break if state is terminal\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab486ca0-2d83-43e2-a3c7-54698c1d0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define first-visit mc prediction function\n",
    "def fv_mc_prediction(policy, env, n_episodes):\n",
    "    # init the empty value table - dictionary to store values of each state\n",
    "    value_table = defaultdict(float)\n",
    "    N = defaultdict(int)\n",
    "\n",
    "    # for each episode generate data and store\n",
    "    for _ in range(n_episodes):\n",
    "        # generate & store data\n",
    "        data = generate_episode_data(policy, env)\n",
    "        returns = 0\n",
    "        if data is not None:\n",
    "            # for each step store rewards and states to temp variable and calculate returns as sum of rewards\n",
    "            # for each episode calculate current value of all the states involved (starting from terminal state)\n",
    "            for i in range(len(data[0]) - 1, -1, -1):\n",
    "                _reward = data[2][i]\n",
    "                _state = data[0][i]\n",
    "\n",
    "                # add reward to returns\n",
    "                returns += _reward\n",
    "                # for first-visit check if episode is visited for the first time\n",
    "                if _state not in data[0][:i]:\n",
    "                    # if yes standard mc incremental equation\n",
    "                    # NewEstimate = OldEstimate + StepSize(Target-OldEstimate)\n",
    "                    N[_state] += 1\n",
    "                    value_table[_state] += (returns - value_table[_state]) / N[_state]\n",
    "            # print(returns)\n",
    "    return value_table\n",
    "\n",
    "\n",
    "prediction = fv_mc_prediction(hit_til_19, env, n_episodes=500000)\n",
    "\n",
    "'''\n",
    "for j in range(10):\n",
    "    print(prediction.popitem())\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c5e06-13ef-4861-a155-943d3db523b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to plot blackjack results\n",
    "def plot_blackjack(p, ax1, ax2):\n",
    "    player_sum = np.arange(12, 21 + 1)\n",
    "    dealer_sum = np.arange(1, 10 + 1)\n",
    "    usable_ace = np.array([False, True])\n",
    "    state_values = np.zeros((len(player_sum), len(dealer_sum), len(usable_ace)))\n",
    "    # loop through player_sum values\n",
    "    for i, player in enumerate(player_sum):\n",
    "        # loop through dealer_sum values\n",
    "        for j, dealer in enumerate(dealer_sum):\n",
    "            # loop through usable_ace values\n",
    "            for k, ace in enumerate(usable_ace):\n",
    "                # create state_value\n",
    "                state_values[i, j, k] = p[player, dealer, ace]\n",
    "\n",
    "    # X, Y coordinates from player and dealer sums\n",
    "    X, Y = np.meshgrid(player_sum, dealer_sum)\n",
    "    ax1.plot_wireframe(X, Y, state_values[:, :, 0])\n",
    "    ax2.plot_wireframe(X, Y, state_values[:, :, 1])\n",
    "\n",
    "    for ax in ax1, ax2:\n",
    "        ax.set_zlim(-1, 1)\n",
    "        ax.set_ylabel('player_sum')\n",
    "        ax.set_xlabel('dealer_sum')\n",
    "        ax.set_zlabel('state_value')\n",
    "        \n",
    "\n",
    "fig, axes = pyplot.subplots(nrows=2, figsize=(5, 8), subplot_kw={'projection': '3d'})\n",
    "\n",
    "axes[0].set_title('state-value distribution w/o usable ace')\n",
    "axes[1].set_title('state-value distribution w usable ace')\n",
    "plot_blackjack(prediction, axes[0], axes[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
